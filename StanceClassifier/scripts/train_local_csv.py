#!/usr/bin/env python3
"""
Fine-tune a stance model on local CSV files.

Expected CSV columns by default:
  - source_text
  - reply_text
  - label   (string labels or numeric IDs)
"""

import argparse
import json
import os
from typing import List, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support
import torch
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    DataCollatorWithPadding,
    Trainer,
    TrainingArguments,
)


DEFAULT_LABELS = ["support", "deny", "query", "comment"]


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Train stance model from local CSV files")
    parser.add_argument("--train-csv", required=True, help="Path to train CSV")
    parser.add_argument("--val-csv", default=None, help="Path to validation CSV")
    parser.add_argument("--test-csv", default=None, help="Path to test CSV")
    parser.add_argument(
        "--base-model",
        default="GateNLP/stance-bertweet-target-aware",
        help="HF model ID or local checkpoint path",
    )
    parser.add_argument(
        "--mode",
        choices=["aware", "oblivious"],
        default="aware",
        help="aware: source+reply; oblivious: reply only",
    )
    parser.add_argument("--output-dir", default="./local_stance_model", help="Output model directory")
    parser.add_argument(
        "--labels",
        default=",".join(DEFAULT_LABELS),
        help="Comma-separated label order (defines label IDs)",
    )
    parser.add_argument("--source-col", default="source_text", help="Source text column name")
    parser.add_argument("--reply-col", default="reply_text", help="Reply text column name")
    parser.add_argument("--label-col", default="label", help="Label column name")
    parser.add_argument("--max-length", type=int, default=256, help="Tokenizer max length")
    parser.add_argument("--epochs", type=float, default=3.0, help="Num epochs")
    parser.add_argument("--train-batch-size", type=int, default=16, help="Train batch size per device")
    parser.add_argument("--eval-batch-size", type=int, default=32, help="Eval batch size per device")
    parser.add_argument("--learning-rate", type=float, default=2e-5, help="Learning rate")
    parser.add_argument("--weight-decay", type=float, default=0.01, help="Weight decay")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--logging-steps", type=int, default=50, help="Logging interval")
    parser.add_argument("--no-fp16", action="store_true", help="Disable fp16 even on CUDA")
    return parser.parse_args()


class EncodedDataset:
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {k: v[idx] for k, v in self.encodings.items()}
        item["labels"] = self.labels[idx]
        return item


def _normalize_label(value, label2id, lower_label2id) -> int:
    if pd.isna(value):
        raise ValueError("Missing label (NaN)")

    if isinstance(value, str):
        key = value.strip()
        if not key:
            raise ValueError("Missing label (empty string)")
        if key in label2id:
            return label2id[key]
        low = key.lower()
        if low in lower_label2id:
            return lower_label2id[low]

        # Also support stringified numeric labels, e.g. "0" / "0.0".
        try:
            num = float(key)
            if num.is_integer():
                idx = int(num)
                if 0 <= idx < len(label2id):
                    return idx
        except ValueError:
            pass

        raise ValueError(
            f"Unknown label '{value}'. Expected one of {list(label2id.keys())} or numeric IDs."
        )

    idx = int(value)
    if idx < 0 or idx >= len(label2id):
        raise ValueError(f"Label ID {idx} out of range [0, {len(label2id)-1}]")
    return idx


def load_split(
    csv_path: str,
    source_col: str,
    reply_col: str,
    label_col: str,
    label2id: dict,
) -> Tuple[List[str], List[str], List[int]]:
    df = pd.read_csv(csv_path)
    for col in [source_col, reply_col, label_col]:
        if col not in df.columns:
            raise ValueError(f"Missing column '{col}' in {csv_path}. Found: {list(df.columns)}")

    missing_mask = df[label_col].isna() | (df[label_col].astype(str).str.strip() == "")
    missing_count = int(missing_mask.sum())
    if missing_count > 0:
        missing_rows = (df.index[missing_mask] + 2).tolist()[:10]  # +2 accounts for header + 0-index
        print(
            f"[WARN] {csv_path}: dropping {missing_count} rows with missing '{label_col}'. "
            f"Example CSV line numbers: {missing_rows}"
        )
        df = df.loc[~missing_mask].copy()
        if df.empty:
            raise ValueError(f"All rows in {csv_path} have missing '{label_col}'")

    sources = df[source_col].fillna("").astype(str).tolist()
    replies = df[reply_col].fillna("").astype(str).tolist()
    raw_labels = df[label_col].tolist()
    row_ids = df.index.tolist()

    lower_label2id = {k.lower(): v for k, v in label2id.items()}
    labels = []
    for row_idx, val in zip(row_ids, raw_labels):
        try:
            labels.append(_normalize_label(val, label2id, lower_label2id))
        except Exception as ex:
            raise ValueError(
                f"Invalid label at CSV line {row_idx + 2} in {csv_path}: {val!r}. {ex}"
            ) from ex
    return sources, replies, labels


def tokenize_split(tokenizer, mode: str, sources: List[str], replies: List[str], max_length: int):
    if mode == "aware":
        return tokenizer(
            sources,
            replies,
            truncation="longest_first",
            max_length=max_length,
            return_overflowing_tokens=False,
            verbose=False,
        )
    return tokenizer(
        replies,
        truncation=True,
        max_length=max_length,
        return_overflowing_tokens=False,
        verbose=False,
    )


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    macro_f1 = f1_score(labels, preds, average="macro", zero_division=0)
    weighted_f1 = f1_score(labels, preds, average="weighted", zero_division=0)
    acc = accuracy_score(labels, preds)
    precision, recall, _, _ = precision_recall_fscore_support(
        labels, preds, average="macro", zero_division=0
    )
    return {
        "accuracy": acc,
        "macro_f1": macro_f1,
        "weighted_f1": weighted_f1,
        "macro_precision": precision,
        "macro_recall": recall,
    }


def main():
    args = parse_args()

    labels = [lab.strip() for lab in args.labels.split(",") if lab.strip()]
    if not labels:
        raise ValueError("--labels is empty after parsing")

    label2id = {lab: i for i, lab in enumerate(labels)}
    id2label = {i: lab for lab, i in label2id.items()}

    tokenizer = AutoTokenizer.from_pretrained(args.base_model)
    model = AutoModelForSequenceClassification.from_pretrained(
        args.base_model,
        num_labels=len(labels),
        ignore_mismatched_sizes=True,
    )
    model.config.label2id = label2id
    model.config.id2label = {str(k): v for k, v in id2label.items()}

    # Keep sequence length within model/tokenizer limits to avoid position embedding overflow.
    tokenizer_limit = getattr(tokenizer, "model_max_length", None)
    if tokenizer_limit is None or tokenizer_limit > 100000:
        tokenizer_limit = args.max_length
    model_limit = getattr(model.config, "max_position_embeddings", None)
    if model_limit is None:
        model_limit = args.max_length
    effective_max_length = int(min(args.max_length, tokenizer_limit, model_limit))
    if effective_max_length < 8:
        raise ValueError(
            f"Resolved max length too small ({effective_max_length}). "
            f"tokenizer_limit={tokenizer_limit}, model_limit={model_limit}"
        )
    if effective_max_length != args.max_length:
        print(
            f"[INFO] max_length adjusted: requested={args.max_length}, "
            f"tokenizer_limit={tokenizer_limit}, model_limit={model_limit}, "
            f"using={effective_max_length}"
        )

    tr_sources, tr_replies, tr_labels = load_split(
        args.train_csv, args.source_col, args.reply_col, args.label_col, label2id
    )
    tr_enc = tokenize_split(tokenizer, args.mode, tr_sources, tr_replies, effective_max_length)
    train_ds = EncodedDataset(tr_enc, tr_labels)

    val_ds = None
    if args.val_csv:
        va_sources, va_replies, va_labels = load_split(
            args.val_csv, args.source_col, args.reply_col, args.label_col, label2id
        )
        va_enc = tokenize_split(tokenizer, args.mode, va_sources, va_replies, effective_max_length)
        val_ds = EncodedDataset(va_enc, va_labels)

    test_ds = None
    if args.test_csv:
        te_sources, te_replies, te_labels = load_split(
            args.test_csv, args.source_col, args.reply_col, args.label_col, label2id
        )
        te_enc = tokenize_split(tokenizer, args.mode, te_sources, te_replies, effective_max_length)
        test_ds = EncodedDataset(te_enc, te_labels)

    use_fp16 = (not args.no_fp16) and torch.cuda.is_available()

    if val_ds is not None:
        evaluation_strategy = "epoch"
        save_strategy = "epoch"
        load_best_model_at_end = True
    else:
        evaluation_strategy = "no"
        save_strategy = "epoch"
        load_best_model_at_end = False

    training_args = TrainingArguments(
        output_dir=args.output_dir,
        learning_rate=args.learning_rate,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.train_batch_size,
        per_device_eval_batch_size=args.eval_batch_size,
        weight_decay=args.weight_decay,
        evaluation_strategy=evaluation_strategy,
        save_strategy=save_strategy,
        load_best_model_at_end=load_best_model_at_end,
        metric_for_best_model="macro_f1",
        greater_is_better=True,
        logging_steps=args.logging_steps,
        report_to="none",
        seed=args.seed,
        fp16=use_fp16,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        tokenizer=tokenizer,
        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
        compute_metrics=compute_metrics,
    )

    print(f"Training mode: {args.mode}")
    print(f"Train samples: {len(train_ds)}")
    if val_ds is not None:
        print(f"Val samples  : {len(val_ds)}")
    if test_ds is not None:
        print(f"Test samples : {len(test_ds)}")

    trainer.train()
    trainer.save_model(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)

    with open(os.path.join(args.output_dir, "label_mapping.json"), "w", encoding="utf-8") as out:
        json.dump(
            {
                "labels": labels,
                "label2id": label2id,
                "id2label": {str(k): v for k, v in id2label.items()},
                "mode": args.mode,
                "source_col": args.source_col,
                "reply_col": args.reply_col,
                "label_col": args.label_col,
            },
            out,
            ensure_ascii=False,
            indent=2,
        )

    if val_ds is not None:
        metrics = trainer.evaluate(eval_dataset=val_ds, metric_key_prefix="val")
        print("\nValidation metrics:")
        for key, value in sorted(metrics.items()):
            if key.startswith("val_"):
                print(f"  {key}: {value:.4f}" if isinstance(value, float) else f"  {key}: {value}")

    if test_ds is not None:
        metrics = trainer.evaluate(eval_dataset=test_ds, metric_key_prefix="test")
        print("\nTest metrics:")
        for key, value in sorted(metrics.items()):
            if key.startswith("test_"):
                print(f"  {key}: {value:.4f}" if isinstance(value, float) else f"  {key}: {value}")

    print(f"\nSaved model to: {args.output_dir}")


if __name__ == "__main__":
    main()
